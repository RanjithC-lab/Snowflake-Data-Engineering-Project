In this project, I integrated Snowflake with AWS S3 to load data from S3 buckets into Snowflake using the COPY INTO command. This process really helped me get comfortable with handling cloud data warehousing and transferring data between services effectively.

I also developed a full data pipeline where I worked on extracting raw CSV files, loading them into Snowflake, and performing data transformations. This hands-on experience helped me grasp the ETL (Extract, Transform, Load) processes better and gave me practical exposure.

During the project, I improved my SQL skills by working with large datasets. I created tables, loaded data, performed data transformations like adding flags and updating columns, and ran various analysis queries. This really strengthened my ability to work with relational databases and structured data.

Another key learning was automating tasks with AWS Lambda. I set up Lambda functions to process and unzip files in the S3 bucket, which was a great way to automate repetitive tasks in a cloud environment. It helped me understand how to integrate Lambda into data pipelines for automation.

I also learned how to use Git and GitHub effectively to manage my project. I tracked changes, handled version control, and shared my work, which are all essential skills when collaborating on data engineering projects or showcasing work to employers.

Lastly, I worked through several challenges during the project, like dealing with permissions errors and resolving merge conflicts. This helped me enhance my problem-solving and debugging skills in a cloud-based data engineering setting.

